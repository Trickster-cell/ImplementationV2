{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spikingjelly in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.0.0.14)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spikingjelly) (2.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spikingjelly) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spikingjelly) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spikingjelly) (4.66.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spikingjelly) (0.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spikingjelly) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->spikingjelly) (2.8.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->spikingjelly) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->spikingjelly) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->spikingjelly) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->spikingjelly) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->spikingjelly) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->spikingjelly) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->spikingjelly) (2.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->spikingjelly) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->spikingjelly) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->spikingjelly) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->spikingjelly) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->spikingjelly) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->spikingjelly) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->spikingjelly) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spikingjelly\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchMerging(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, patch_size: int, overlap_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride=overlap_size, padding=patch_size // 2, bias=False)\n",
    "        self.lif = neuron.LIFNode(surrogate_function=surrogate.ATan() ,detach_reset=True, backend='torch', step_mode='m')\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        t, b, c, h, w = x.shape\n",
    "        # print(\"reached\")\n",
    "        x = self.conv(x.flatten(0,1)) #\n",
    "        # print(x)\n",
    "        x = self.lif(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mix_FFN(nn.Module):\n",
    "  '''\n",
    "  dense layer followed by convolution layer with gelu activation then another\n",
    "  dense layer\n",
    "  '''\n",
    "  def __init__(self, channels, expansion=4):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.expansion = expansion\n",
    "\n",
    "    self.dense1 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "    self.lif1 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "    self.conv = nn.Conv2d(channels,\n",
    "                              channels*expansion,\n",
    "                              kernel_size=3,\n",
    "                              groups= channels,\n",
    "                              padding=1)\n",
    "    self.lif2 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "    self.dense2 = nn.Conv2d(channels*expansion, channels, kernel_size=1)\n",
    "    self.lif3 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "  def forward(self,x):\n",
    "    t, b, c, h, w = x.shape\n",
    "    x=self.dense1(x.flatten(0,1))\n",
    "    x=self.lif1(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "\n",
    "    x=self.conv(x.flatten(0,1))\n",
    "    x=self.lif2(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    \n",
    "    x=self.dense2(x.flatten(0,1))\n",
    "    x=self.lif3(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    #print(\"ffn\",x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#from config import *\n",
    "\n",
    "class SSA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads): #dmodel will be embed dimensions\n",
    "        super(SSA, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = 0.125\n",
    "\n",
    "        # Linear transformations for query, key, and value\n",
    "        self.W_q = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1,bias=False)\n",
    "        self.q_lif = neuron.LIFNode(tau=2.0, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        self.W_k = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1,bias=False)\n",
    "        self.k_lif = neuron.LIFNode(tau=2.0, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        self.W_v = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1,bias=False)\n",
    "        self.v_lif = neuron.LIFNode(tau=2.0, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        # Linear transformation for output\n",
    "        self.W_o = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1)\n",
    "        self.o_lif = neuron.LIFNode(tau=2.0, v_threshold=0.5, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        self.proj= nn.Conv1d(d_model, d_model, stride=1, kernel_size=1)\n",
    "        self.proj_lif = neuron.LIFNode(tau=2.0, detach_reset=True,step_mode='m', surrogate_function=surrogate.ATan())\n",
    "       \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate the dot product of query and key matrices\n",
    "        ktv = torch.matmul(K.transpose(-2, -1),V)\n",
    "        out= torch.matmul(Q,ktv)*self.scale\n",
    "        \n",
    "        # if mask is not None:\n",
    "        #     scores += mask * -1e9\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self, Q, K, V, t, b):\n",
    "        # Linear transformations\n",
    "        #q,k,v are 3d tensors t*b, c, h*w\n",
    "        Q = self.W_q(Q)\n",
    "        # print(\"reached\")\n",
    "        Q = self.q_lif(Q.reshape(t,b,Q.shape[1],Q.shape[2])) \n",
    "        # print(\"reached2\")\n",
    "        K = self.W_k(K)\n",
    "        K = self.k_lif(K.reshape(t,b,K.shape[1],K.shape[2]))\n",
    "        V = self.W_v(V)\n",
    "        V = self.v_lif(V.reshape(t,b,V.shape[1],V.shape[2]))\n",
    "        \n",
    "        #4d to 3d\n",
    "        Q = Q.flatten(0,1)\n",
    "        K = K.flatten(0,1)\n",
    "        V = V.flatten(0,1)\n",
    "\n",
    "        \n",
    "        # Splitting into multiple heads\n",
    "        Q = Q.view(Q.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        K = K.view(K.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        V = V.view(V.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        output= self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Concatenate attention outputs of all heads and apply final linear transformation\n",
    "        output = output.transpose(1, 2).contiguous().view(output.size(0),self.d_model,-1)\n",
    "        output = self.W_o(output)\n",
    "        #print(\"output\", output.shape)\n",
    "        output = self.o_lif(output.reshape(t,b,output.shape[1], output.shape[2]))\n",
    "        #print(\"output\", output.shape)\n",
    "        projection = self.proj(output.flatten(0,1))\n",
    "        #print(\"proj\", projection.shape)\n",
    "        projection = self.proj_lif(projection.reshape(t,b,projection.shape[1],projection.shape[2]))\n",
    "        \n",
    "        # print(\"testtt\", projection.shape)\n",
    "        \n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Rearrange(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return x.permute(0, 2, 3, 1)\n",
    "\n",
    "# class RearrangeBack(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return x.permute(0, 3, 1, 2)\n",
    "\n",
    "class EfficientMultiHeadedAttention(nn.Module):\n",
    "  def __init__(self, channels, reduction_ratio, num_heads):\n",
    "    super().__init__()\n",
    "    self.conv=nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride=reduction_ratio)\n",
    "    self.lif=neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "    self.att = SSA(d_model = channels, num_heads=num_heads)\n",
    "\n",
    "  def forward(self, x):\n",
    "    t, batch_size, chnls, h, w = x.shape\n",
    "    # print(\"initial shape\",x.shape)\n",
    "    \n",
    "    reduced_x=self.conv(x.flatten(0,1))#t*b c h w\n",
    "    reduced_x = self.lif(reduced_x.reshape(t,batch_size,reduced_x.shape[1],reduced_x.shape[2],reduced_x.shape[3]))#t b c h w\n",
    "    # print(\"reduced_x\", reduced_x.shape)\n",
    "    reduced_x = reduced_x.flatten(0,1)#t*b c h w\n",
    "    reduced_x = reduced_x.reshape(reduced_x.size(0),reduced_x.size(1),-1)#t*b c h*w \n",
    "    \n",
    "    x=x.flatten(0,1)\n",
    "    x = x.reshape(x.size(0),x.size(1),-1)#t*b c h*w\n",
    "    #print(\"final x\", x.shape)\n",
    "    #print(\"red_x\", reduced_x.shape)\n",
    "    out = self.att(x, reduced_x, reduced_x,t,batch_size)\n",
    "    # out = out.reshape(out.size(0), out.size(2), h, w)\n",
    "    # out = out.reshape(out.size(0), out.size(1), h, w)\n",
    "    out=out.reshape(out.size(0), out.size(1), out.size(2), h, w)\n",
    "    #print(\"ema\",out.shape)\n",
    "    \n",
    "    return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "  def __init__(self, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn\n",
    "\n",
    "  def forward(self, x, **kwargs):\n",
    "    out = self.fn(x, **kwargs)\n",
    "    x=x+out\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerEncoderBlock(nn.Sequential):\n",
    "  def __init__(\n",
    "      self,\n",
    "      channels: int,\n",
    "      reduction_ratio: int = 1,\n",
    "      num_heads: int = 8,\n",
    "      mlp_expansion: int = 4,\n",
    "      drop_path_prob: float = 0.0,\n",
    "  ):\n",
    "\n",
    "    super().__init__(\n",
    "        ResidualAdd(\n",
    "            nn.Sequential(\n",
    "                EfficientMultiHeadedAttention(channels, reduction_ratio, num_heads),\n",
    "            )\n",
    "        ),\n",
    "        ResidualAdd(\n",
    "            nn.Sequential(\n",
    "                Mix_FFN(channels, expansion = mlp_expansion),\n",
    "                StochasticDepth(p=drop_path_prob, mode=\"batch\")\n",
    "            )\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class SegFormerEncoderStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        patch_size: int,\n",
    "        overlap_size: int,\n",
    "        drop_probs: List[int],\n",
    "        depth: int = 2,\n",
    "        reduction_ratio: int = 1,\n",
    "        num_heads: int = 8,\n",
    "        mlp_expansion: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.overlap_patch_merge = OverlapPatchMerging(\n",
    "            in_channels, out_channels, patch_size, overlap_size,\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                SegFormerEncoderBlock(\n",
    "                    out_channels, reduction_ratio, num_heads, mlp_expansion, drop_probs[i]\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "def chunks(data: Iterable, sizes):\n",
    "  curr=0\n",
    "  for size in sizes:\n",
    "    chunk = data[curr: curr+size]\n",
    "    curr+=size\n",
    "    yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerEncoder(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      in_channels,\n",
    "      widths,\n",
    "      depths,\n",
    "      all_num_heads,\n",
    "      patch_sizes,\n",
    "      overlap_sizes,\n",
    "      reduction_ratios,\n",
    "      mlp_expansions,\n",
    "      drop_prob = 0.0,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    drop_probs = [x.item() for x in torch.linspace(0,drop_prob,sum(depths))]\n",
    "    self.stages = nn.ModuleList(\n",
    "        [\n",
    "            SegFormerEncoderStage(*args)\n",
    "            for args in zip(\n",
    "                [in_channels, *widths],\n",
    "                widths,\n",
    "                patch_sizes,\n",
    "                overlap_sizes,\n",
    "                chunks(drop_probs, sizes=depths),\n",
    "                depths,\n",
    "                reduction_ratios,\n",
    "                all_num_heads,\n",
    "                mlp_expansions\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    features = []\n",
    "    for stage in self.stages:\n",
    "      x=stage(x)\n",
    "      features.append(x)\n",
    "    return features\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, scale_factor: int =2):\n",
    "    super().__init__()\n",
    "    self.upsample=nn.UpsamplingBilinear2d(scale_factor = scale_factor)\n",
    "    self.conv= nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
    "    self.lif= neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "  def forward(self,x):\n",
    "    t, b, c, h, w = x.shape\n",
    "    x=self.upsample(x.flatten(0,1))\n",
    "    x=self.conv(x) #b*t *\n",
    "    x=self.lif(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, out_channels:int , widths: List[int], scale_factors: List[int]):\n",
    "    super().__init__()\n",
    "    self.stages = nn.ModuleList(\n",
    "        [\n",
    "            DecoderBlock(in_channels, out_channels, scale_factor)\n",
    "            for in_channels, scale_factor in zip(widths, scale_factors)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def forward(self, features):\n",
    "    new_features = []\n",
    "    for feature, stage in zip(features, self.stages):\n",
    "      x=stage(feature)\n",
    "      new_features.append(x)\n",
    "\n",
    "    return new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "  def __init__(self, channels, num_classes, num_features = 4):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.num_classes = num_classes\n",
    "    self.num_features = num_features\n",
    "    self.dense1 = nn.Conv2d(channels*num_features, channels, kernel_size=1, bias=False)\n",
    "    self.lif1= neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "    self.predict = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "    self.upscale = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "    self.lif2 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "  def forward(self,x):\n",
    "    print(x[0].shape)\n",
    "    x=torch.cat(x, dim=1)\n",
    "    t, b, c, h, w = x.shape\n",
    "    print(x.shape) #3 64 \n",
    "    x = x.flatten(0,1)\n",
    "    # x = x.reshape(x.shape[1], x.shape[0], x.shape[2], x.shape[3])\n",
    "    x=self.dense1(x)\n",
    "    print('y1')\n",
    "    x=self.lif1(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    x=self.predict(x.flatten(0,1))\n",
    "    x=self.lif2(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    x=self.upscale(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps,\n",
    "        in_channels,\n",
    "        widths,\n",
    "        depths,\n",
    "        all_num_heads,\n",
    "        patch_sizes,\n",
    "        overlap_sizes,\n",
    "        reduction_ratios,\n",
    "        mlp_expansions,\n",
    "        decoder_channels,\n",
    "        scale_factors,\n",
    "        num_classes,\n",
    "        drop_prob : float = 0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = SegFormerEncoder(\n",
    "            in_channels,\n",
    "            widths,\n",
    "            depths,\n",
    "            all_num_heads,\n",
    "            patch_sizes,\n",
    "            overlap_sizes,\n",
    "            reduction_ratios,\n",
    "            mlp_expansions,\n",
    "            drop_prob,\n",
    "        )\n",
    "        self.decoder = Decoder(decoder_channels, widths[::-1], scale_factors)\n",
    "        self.seghead = SegmentationHead(\n",
    "            decoder_channels, num_classes, num_features=len(widths)\n",
    "        )\n",
    "\n",
    "        functional.set_step_mode(self,'m')\n",
    "        #if use_cupy:\n",
    "        #functional.set_backend(self, backend='cupy')\n",
    "\n",
    "        self.steps = steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = (x.unsqueeze(0)).repeat(self.steps, 1, 1, 1, 1)\n",
    "        print(\"img\",x.shape)\n",
    "        features = self.encoder(x)\n",
    "        print(\"enc\", features[0].shape)\n",
    "        features = self.decoder(features[::-1])\n",
    "        print(\"dec\", features[0].shape)\n",
    "        segmentation = self.seghead(features)\n",
    "        print(\"seg\", segmentation.shape)\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "# from config import num_steps, batch_size\n",
    "import torch\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "num_steps = 3\n",
    "batch_size = 16\n",
    "\n",
    "#these classes are going to be segmentated total 20 classes\n",
    "ignore_index = 255\n",
    "void_classes= [0,1,2,3,4,5,6,9,10,14,15,16,18,29,30,-1]\n",
    "valid_classes= [ignore_index, 7,8,11,12,13,17,19,20,21,22,23,24,25,26,27,28,31,32,33]\n",
    "class_names = ['unlabeled',\n",
    "               'road',\n",
    "               'sidewalk',\n",
    "               'building',\n",
    "               'wall',\n",
    "               'fence',\n",
    "               'pole',\n",
    "               'traffic light',\n",
    "               'traffic sign',\n",
    "               'vegetation',\n",
    "               'terrain',\n",
    "               'sky',\n",
    "               'person',\n",
    "               'rider',\n",
    "               'car',\n",
    "               'truck',\n",
    "               'bus',\n",
    "               'train',\n",
    "               'motorcycle',\n",
    "               'bicycle']\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes = len(valid_classes)\n",
    "\n",
    "#in later stage we might want to get the segements as different color coded so this is for it\n",
    "colors =[\n",
    "    [  0,   0,   0],\n",
    "    [128,  64, 128],\n",
    "    [244,  35, 232],\n",
    "    [220,  20,  60],\n",
    "    [255,   0,   0],\n",
    "    [  0,   0, 142],\n",
    "    [  0,   0,  70],\n",
    "    [  0,  60, 100],\n",
    "    [  0,  80, 100],\n",
    "    [  0,   0, 230],\n",
    "    [119,  11,  32],\n",
    "]\n",
    "\n",
    "label_colors = dict(zip(range(n_classes), colors))\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    '''\n",
    "    online mila tha \n",
    "    remove unwanted classes and rectify the labels of wanted classes\n",
    "    '''\n",
    "    for void_c in void_classes:\n",
    "        mask[mask == void_c] = ignore_index\n",
    "    for valid_c in valid_classes:\n",
    "        mask[mask == valid_c] = class_map[valid_c]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def decode_segmap(temp):\n",
    "    '''\n",
    "    ye bhi online mila tha\n",
    "    convert greyscale to color\n",
    "    ye to use nahi kara hai \n",
    "    '''\n",
    "    temp = temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colors[l][0] \n",
    "        g[temp == l] = label_colors[l][1] \n",
    "        b[temp == l] = label_colors[l][2]\n",
    "    \n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = r/255.0 \n",
    "    rgb[:,:,1] = g/255.0 \n",
    "    rgb[:,:,2] = b/255.0 \n",
    "    \n",
    "    return rgb\n",
    "\n",
    "\n",
    "\n",
    "class AdjustGamma:\n",
    "    '''\n",
    "    image bohot dark aa rahi thi to ye laga diya hai\n",
    "    thoda washed ho gayi hai image par iski wajah se\n",
    "    '''\n",
    "    def __init__(self, gamma, gain=1):\n",
    "        self.gamma = gamma\n",
    "        self.gain = gain\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        img = np.transpose(image,(2,0,1))\n",
    "        gamma_tensor = torchvision.transforms.functional.adjust_gamma(torch.from_numpy(img), self.gamma, self.gain)\n",
    "        img = np.transpose(gamma_tensor.numpy(), (1,2,0))\n",
    "        return {'image': img, 'mask': mask}\n",
    "\n",
    "class SpikeEncoding:\n",
    "    '''\n",
    "    rate coding ka code\n",
    "    '''\n",
    "    def __call__(self, image, mask):\n",
    "        image = image.float()  # Convert to float\n",
    "        mask = mask.float()    # Convert to float\n",
    "        out_img = spikegen.rate(image, num_steps=num_steps)\n",
    "        out_mask = spikegen.rate(mask, num_steps=num_steps)\n",
    "        # out_img = out_img.bool()      #IMPORTNAT BUT NOT IMPLEMENTED\n",
    "        # out_mask = out_mask.bool()\n",
    "        return {'image': out_img, 'mask': out_mask}\n",
    "    \n",
    "class normalizeSeg:\n",
    "    '''\n",
    "    segmap me pata nahi normalized values nahi aa rahi thi\n",
    "    to ye ek alag se bhi bana diya \n",
    "    '''\n",
    "    def __call__(self, image, mask):\n",
    "        normalized_seg = (mask-torch.min(mask))/(torch.max(mask)-torch.min(mask))\n",
    "        return {'image': image, 'mask': normalized_seg}\n",
    "\n",
    "class encode:\n",
    "    '''\n",
    "    upar wala encode segmap function use kara hai mask par\n",
    "    '''\n",
    "    def __call__(self, image, mask):\n",
    "        final = encode_segmap(mask)\n",
    "        return {'image': image, 'mask': final}\n",
    "'''\n",
    "yaha par transforms hai \n",
    "'''\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(224,224),\n",
    "        AdjustGamma(gamma=0.63),\n",
    "        A.Normalize(mean = (0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225), max_pixel_value = float(225)),\n",
    "        ToTensorV2(),\n",
    "        encode(),\n",
    "        normalizeSeg(),\n",
    "        SpikeEncoding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "class data_transform(Cityscapes):\n",
    "    '''\n",
    "    isko samajhne ki koi zarurat nahi \n",
    "    waise bhi source code uthaya hai bas\n",
    "    par ye wala dono (image, segmap ) ko return karta hai ek saath\n",
    "    '''\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        \n",
    "        targets: Any = []\n",
    "        for i,t in enumerate(self.target_type):\n",
    "            if t == 'polygon':\n",
    "                target = self.load_json(self.targets[index][i])\n",
    "            else:\n",
    "                target = Image.open(self.targets[index][i])\n",
    "            targets.append(target)\n",
    "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
    "        \n",
    "\n",
    "        if self.transforms is not None :\n",
    "            transformed=transform(image=np.array(image), mask=np.array(target))\n",
    "            return transformed['image'], transformed['mask']\n",
    "        return image, target\n",
    "\n",
    "'''\n",
    "aise data aa ajyega\n",
    "\n",
    "dataset = data_transform('/media/iitp/ACER DATA1/cityscapes', split='val', mode='fine', target_type='semantic', transforms=transform)\n",
    "img, seg = dataset[20]\n",
    "print(img.shape, seg.shape)\n",
    "\n",
    "print(img.sum())\n",
    "print(seg.sum())\n",
    "print(img.dtype)\n",
    "print(seg.dtype)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "ch=['r','g','b']\n",
    "for i in range (0,3):\n",
    "    img_sample = img[:, i]\n",
    "    #print(img.size())\n",
    "    fig, ax = plt.subplots()\n",
    "    anim = splt.animator(img_sample, fig, ax)\n",
    "    HTML(anim.to_html5_video())\n",
    "    anim.save(f\"spike_mnist_{ch[i]}.gif\")\n",
    "\n",
    "\n",
    "mask_sample = seg[:]\n",
    "fig, ax = plt.subplots()\n",
    "anim = splt.animator(mask_sample, fig, ax)\n",
    "HTML(anim.to_html5_video())\n",
    "anim.save(f\"mask_sample.gif\")\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class CustomCollate:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, batch):\n",
    "       # Assuming batch is a list of tensors [image, mask]\n",
    "       images = [item[0] for item in batch]\n",
    "       masks = [item[1] for item in batch]\n",
    "       \n",
    "       # Stack images and masks along dimension 1\n",
    "       images_stacked = torch.stack(images, dim=1)\n",
    "       masks_stacked = torch.stack(masks, dim=1)\n",
    "       \n",
    "       return [images_stacked, masks_stacked]\n",
    "\n",
    "\n",
    "class GetData(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self, stage= None):\n",
    "        self.train_dataset = data_transform(root='../cityscapes', split='train', mode='fine', target_type='semantic', transforms=transform)\n",
    "        self.val_dataset = data_transform(root='../cityscapes', split='val', mode='fine', target_type='semantic', transforms=transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=CustomCollate(batch_size=batch_size))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=CustomCollate(batch_size=batch_size))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader ready\n",
      " number of batches = 186\n",
      "val_dataloader ready\n",
      " number of batches = 32\n",
      "img torch.Size([3, 16, 3, 224, 224])\n",
      "enc torch.Size([3, 16, 64, 56, 56])\n",
      "dec torch.Size([3, 16, 256, 56, 56])\n",
      "torch.Size([3, 16, 256, 56, 56])\n",
      "torch.Size([3, 64, 256, 56, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 1024, 1, 1], expected input[256, 192, 56, 56] to have 1024 channels, but got 192 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\BTP\\ImplementationV2\\jelly.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m spk_rec \u001b[39m=\u001b[39m model(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(spk_rec\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# print(spk_rec)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\Downloads\\BTP\\ImplementationV2\\jelly.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(features[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdec\u001b[39m\u001b[39m\"\u001b[39m, features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m segmentation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseghead(features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mseg\u001b[39m\u001b[39m\"\u001b[39m, segmentation\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m segmentation\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\Downloads\\BTP\\ImplementationV2\\jelly.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39my1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/BTP/ImplementationV2/jelly.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlif1(x\u001b[39m.\u001b[39mreshape(t, b, x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 1024, 1, 1], expected input[256, 192, 56, 56] to have 1024 channels, but got 192 channels instead"
     ]
    }
   ],
   "source": [
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "import torch\n",
    "import torch.utils.tensorboard as tb\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "GetData_obj = GetData(batch_size=batch_size)\n",
    "GetData_obj.setup()\n",
    "train_loader = GetData_obj.train_dataloader()\n",
    "val_loader = GetData_obj.val_dataloader()\n",
    "\n",
    "\n",
    "print(f\"train_dataloader ready\\n number of batches = {len(train_loader)}\")\n",
    "print(f\"val_dataloader ready\\n number of batches = {len(val_loader)}\")\n",
    "\n",
    "model = SegFormer(\n",
    "    steps= 4,\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=20,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start_time = time.time() #i guess it is for showing the final time taken\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in iter(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        spk_rec = model(img)\n",
    "        print(spk_rec.shape)\n",
    "        # print(spk_rec)\n",
    "        print(label.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # out_fr = 0\n",
    "        # for t in range(num_steps): #args.T\n",
    "        #     out_fr += model(img)\n",
    "\n",
    "        # out_fr = out_fr / num_steps\n",
    "        # loss = F.mse_loss(out_fr, label_onehot)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        #train_samples += label.numel()\n",
    "        #train_loss += loss.item() * label.numel()\n",
    "        # The correct rate is calculated as follows. The subscript i of the neuron with the highest firing rate in the output layer is considered as the result of classification.\n",
    "        #train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        # After optimizing the parameters, the state of the network should be reset because the neurons of the SNN have memory.\n",
    "        functional.reset_net(model)\n",
    "\n",
    "#CHECKPOINTS NOT IMPLEMENTED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
