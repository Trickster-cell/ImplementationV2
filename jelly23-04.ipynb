{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spikingjelly\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchMerging(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, patch_size: int, overlap_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride=overlap_size, padding=patch_size // 2, bias=False)\n",
    "        self.lif = neuron.LIFNode(surrogate_function=surrogate.ATan() ,detach_reset=True, backend='torch', step_mode='m')\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        t, b, c, h, w = x.shape\n",
    "        # print(\"reached\")\n",
    "        x = self.conv(x.flatten(0,1)) #\n",
    "        # print(x)\n",
    "        x = self.lif(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mix_FFN(nn.Module):\n",
    "  '''\n",
    "  dense layer followed by convolution layer with gelu activation then another\n",
    "  dense layer\n",
    "  '''\n",
    "  def __init__(self, channels, expansion=4):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.expansion = expansion\n",
    "\n",
    "    self.dense1 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "    self.lif1 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "    self.conv = nn.Conv2d(channels,\n",
    "                              channels*expansion,\n",
    "                              kernel_size=3,\n",
    "                              groups= channels,\n",
    "                              padding=1)\n",
    "    self.lif2 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "    self.dense2 = nn.Conv2d(channels*expansion, channels, kernel_size=1)\n",
    "    self.lif3 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "  def forward(self,x):\n",
    "    t, b, c, h, w = x.shape\n",
    "    x=self.dense1(x.flatten(0,1))\n",
    "    x=self.lif1(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "\n",
    "    x=self.conv(x.flatten(0,1))\n",
    "    x=self.lif2(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    \n",
    "    x=self.dense2(x.flatten(0,1))\n",
    "    x=self.lif3(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    #print(\"ffn\",x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#from config import *\n",
    "\n",
    "class SSA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads): #dmodel will be embed dimensions\n",
    "        super(SSA, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = 0.125\n",
    "\n",
    "        # Linear transformations for query, key, and value\n",
    "        self.W_q = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1,bias=False)\n",
    "        self.q_lif = neuron.LIFNode(tau=2.0, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        self.W_k = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1,bias=False)\n",
    "        self.k_lif = neuron.LIFNode(tau=2.0, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        self.W_v = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1,bias=False)\n",
    "        self.v_lif = neuron.LIFNode(tau=2.0, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        # Linear transformation for output\n",
    "        self.W_o = nn.Conv1d(d_model, d_model, kernel_size=1, stride=1)\n",
    "        self.o_lif = neuron.LIFNode(tau=2.0, v_threshold=0.5, detach_reset=True, step_mode='m', surrogate_function=surrogate.ATan())\n",
    "        self.proj= nn.Conv1d(d_model, d_model, stride=1, kernel_size=1)\n",
    "        self.proj_lif = neuron.LIFNode(tau=2.0, detach_reset=True,step_mode='m', surrogate_function=surrogate.ATan())\n",
    "       \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate the dot product of query and key matrices\n",
    "        ktv = torch.matmul(K.transpose(-2, -1),V)\n",
    "        out= torch.matmul(Q,ktv)*self.scale\n",
    "        \n",
    "        # if mask is not None:\n",
    "        #     scores += mask * -1e9\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self, Q, K, V, t, b):\n",
    "        # Linear transformations\n",
    "        #q,k,v are 3d tensors t*b, c, h*w\n",
    "        Q = self.W_q(Q)\n",
    "        # print(\"reached\")\n",
    "        Q = self.q_lif(Q.reshape(t,b,Q.shape[1],Q.shape[2])) \n",
    "        # print(\"reached2\")\n",
    "        K = self.W_k(K)\n",
    "        K = self.k_lif(K.reshape(t,b,K.shape[1],K.shape[2]))\n",
    "        V = self.W_v(V)\n",
    "        V = self.v_lif(V.reshape(t,b,V.shape[1],V.shape[2]))\n",
    "        \n",
    "        #4d to 3d\n",
    "        Q = Q.flatten(0,1)\n",
    "        K = K.flatten(0,1)\n",
    "        V = V.flatten(0,1)\n",
    "\n",
    "        \n",
    "        # Splitting into multiple heads\n",
    "        Q = Q.view(Q.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        K = K.view(K.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        V = V.view(V.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        output= self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Concatenate attention outputs of all heads and apply final linear transformation\n",
    "        output = output.transpose(1, 2).contiguous().view(output.size(0),self.d_model,-1)\n",
    "        output = self.W_o(output)\n",
    "        #print(\"output\", output.shape)\n",
    "        output = self.o_lif(output.reshape(t,b,output.shape[1], output.shape[2]))\n",
    "        #print(\"output\", output.shape)\n",
    "        projection = self.proj(output.flatten(0,1))\n",
    "        #print(\"proj\", projection.shape)\n",
    "        projection = self.proj_lif(projection.reshape(t,b,projection.shape[1],projection.shape[2]))\n",
    "        \n",
    "        # print(\"testtt\", projection.shape)\n",
    "        \n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Rearrange(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return x.permute(0, 2, 3, 1)\n",
    "\n",
    "# class RearrangeBack(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return x.permute(0, 3, 1, 2)\n",
    "\n",
    "class EfficientMultiHeadedAttention(nn.Module):\n",
    "  def __init__(self, channels, reduction_ratio, num_heads):\n",
    "    super().__init__()\n",
    "    self.conv=nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=reduction_ratio, stride=reduction_ratio)\n",
    "    self.lif=neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "    self.att = SSA(d_model = channels, num_heads=num_heads)\n",
    "\n",
    "  def forward(self, x):\n",
    "    t, batch_size, chnls, h, w = x.shape\n",
    "    # print(\"initial shape\",x.shape)\n",
    "    \n",
    "    reduced_x=self.conv(x.flatten(0,1))#t*b c h w\n",
    "    reduced_x = self.lif(reduced_x.reshape(t,batch_size,reduced_x.shape[1],reduced_x.shape[2],reduced_x.shape[3]))#t b c h w\n",
    "    # print(\"reduced_x\", reduced_x.shape)\n",
    "    reduced_x = reduced_x.flatten(0,1)#t*b c h w\n",
    "    reduced_x = reduced_x.reshape(reduced_x.size(0),reduced_x.size(1),-1)#t*b c h*w \n",
    "    \n",
    "    x=x.flatten(0,1)\n",
    "    x = x.reshape(x.size(0),x.size(1),-1)#t*b c h*w\n",
    "    #print(\"final x\", x.shape)\n",
    "    #print(\"red_x\", reduced_x.shape)\n",
    "    out = self.att(x, reduced_x, reduced_x,t,batch_size)\n",
    "    # out = out.reshape(out.size(0), out.size(2), h, w)\n",
    "    # out = out.reshape(out.size(0), out.size(1), h, w)\n",
    "    out=out.reshape(out.size(0), out.size(1), out.size(2), h, w)\n",
    "    #print(\"ema\",out.shape)\n",
    "    \n",
    "    return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "  def __init__(self, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn\n",
    "\n",
    "  def forward(self, x, **kwargs):\n",
    "    out = self.fn(x, **kwargs)\n",
    "    x=x+out\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerEncoderBlock(nn.Sequential):\n",
    "  def __init__(\n",
    "      self,\n",
    "      channels: int,\n",
    "      reduction_ratio: int = 1,\n",
    "      num_heads: int = 8,\n",
    "      mlp_expansion: int = 4,\n",
    "      drop_path_prob: float = 0.0,\n",
    "  ):\n",
    "\n",
    "    super().__init__(\n",
    "        ResidualAdd(\n",
    "            nn.Sequential(\n",
    "                EfficientMultiHeadedAttention(channels, reduction_ratio, num_heads),\n",
    "            )\n",
    "        ),\n",
    "        ResidualAdd(\n",
    "            nn.Sequential(\n",
    "                Mix_FFN(channels, expansion = mlp_expansion),\n",
    "                StochasticDepth(p=drop_path_prob, mode=\"batch\")\n",
    "            )\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class SegFormerEncoderStage(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        patch_size: int,\n",
    "        overlap_size: int,\n",
    "        drop_probs: List[int],\n",
    "        depth: int = 2,\n",
    "        reduction_ratio: int = 1,\n",
    "        num_heads: int = 8,\n",
    "        mlp_expansion: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.overlap_patch_merge = OverlapPatchMerging(\n",
    "            in_channels, out_channels, patch_size, overlap_size,\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                SegFormerEncoderBlock(\n",
    "                    out_channels, reduction_ratio, num_heads, mlp_expansion, drop_probs[i]\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "def chunks(data: Iterable, sizes):\n",
    "  curr=0\n",
    "  for size in sizes:\n",
    "    chunk = data[curr: curr+size]\n",
    "    curr+=size\n",
    "    yield chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerEncoder(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      in_channels,\n",
    "      widths,\n",
    "      depths,\n",
    "      all_num_heads,\n",
    "      patch_sizes,\n",
    "      overlap_sizes,\n",
    "      reduction_ratios,\n",
    "      mlp_expansions,\n",
    "      drop_prob = 0.0,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    drop_probs = [x.item() for x in torch.linspace(0,drop_prob,sum(depths))]\n",
    "    self.stages = nn.ModuleList(\n",
    "        [\n",
    "            SegFormerEncoderStage(*args)\n",
    "            for args in zip(\n",
    "                [in_channels, *widths],\n",
    "                widths,\n",
    "                patch_sizes,\n",
    "                overlap_sizes,\n",
    "                chunks(drop_probs, sizes=depths),\n",
    "                depths,\n",
    "                reduction_ratios,\n",
    "                all_num_heads,\n",
    "                mlp_expansions\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    features = []\n",
    "    for stage in self.stages:\n",
    "      x=stage(x)\n",
    "      features.append(x)\n",
    "    return features\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, scale_factor: int =2):\n",
    "    super().__init__()\n",
    "    self.upsample=nn.UpsamplingBilinear2d(scale_factor = scale_factor)\n",
    "    self.conv= nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
    "    self.lif= neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "  def forward(self,x):\n",
    "    t, b, c, h, w = x.shape\n",
    "    x=self.upsample(x.flatten(0,1))\n",
    "    x=self.conv(x) #b*t *\n",
    "    x=self.lif(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, out_channels:int , widths: List[int], scale_factors: List[int]):\n",
    "    super().__init__()\n",
    "    self.stages = nn.ModuleList(\n",
    "        [\n",
    "            DecoderBlock(in_channels, out_channels, scale_factor)\n",
    "            for in_channels, scale_factor in zip(widths, scale_factors)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def forward(self, features):\n",
    "    new_features = []\n",
    "    for feature, stage in zip(features, self.stages):\n",
    "      x=stage(feature)\n",
    "      new_features.append(x)\n",
    "\n",
    "    return new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "  def __init__(self, channels, num_classes, num_features = 4):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.num_classes = num_classes\n",
    "    self.num_features = num_features\n",
    "    self.dense1 = nn.Conv2d(channels*num_features, channels, kernel_size=1, bias=False)\n",
    "    self.lif1= neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "    self.predict = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "    self.upscale = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "    self.lif2 = neuron.LIFNode(surrogate_function=surrogate.ATan(), detach_reset=True, backend='torch', step_mode='m')\n",
    "\n",
    "  def forward(self,x):\n",
    "    #print(x[0].shape)\n",
    "    #for tensor in x:\n",
    "    #  print(tensor.shape)\n",
    "    x=torch.cat(x, dim=2)\n",
    "    t, b, c, h, w = x.shape\n",
    "    #print(x.shape) \n",
    "    x = x.flatten(0,1)\n",
    "    # x = x.reshape(x.shape[1], x.shape[0], x.shape[2], x.shape[3])\n",
    "    x=self.dense1(x)\n",
    "    #print('y1')\n",
    "    x=self.lif1(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "    x=self.predict(x.flatten(0,1))\n",
    "    x=self.upscale(x)\n",
    "    x=self.lif2(x.reshape(t, b, x.shape[1], x.shape[2], x.shape[3]))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps,\n",
    "        in_channels,\n",
    "        widths,\n",
    "        depths,\n",
    "        all_num_heads,\n",
    "        patch_sizes,\n",
    "        overlap_sizes,\n",
    "        reduction_ratios,\n",
    "        mlp_expansions,\n",
    "        decoder_channels,\n",
    "        scale_factors,\n",
    "        num_classes,\n",
    "        drop_prob : float = 0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = SegFormerEncoder(\n",
    "            in_channels,\n",
    "            widths,\n",
    "            depths,\n",
    "            all_num_heads,\n",
    "            patch_sizes,\n",
    "            overlap_sizes,\n",
    "            reduction_ratios,\n",
    "            mlp_expansions,\n",
    "            drop_prob,\n",
    "        )\n",
    "        self.decoder = Decoder(decoder_channels, widths[::-1], scale_factors)\n",
    "        self.seghead = SegmentationHead(\n",
    "            decoder_channels, num_classes, num_features=len(widths)\n",
    "        )\n",
    "\n",
    "        functional.set_step_mode(self,'m')\n",
    "        #if use_cupy:\n",
    "        #functional.set_backend(self, backend='cupy')\n",
    "\n",
    "        self.steps = steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = (x.unsqueeze(0)).repeat(self.steps, 1, 1, 1, 1)\n",
    "        #print(\"img\",x.shape)\n",
    "        features = self.encoder(x)\n",
    "        #print(\"enc\", features[0].shape)\n",
    "        features = self.decoder(features[::-1])\n",
    "        #print(\"dec\", features[0].shape)\n",
    "        segmentation = self.seghead(features)\n",
    "        #print(\"segmentaion mask:\", segmentation.shape)\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "# from config import num_steps, batch_size\n",
    "import torch\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "num_steps = 3\n",
    "batch_size = 16\n",
    "\n",
    "#these classes are going to be segmentated total 20 classes\n",
    "ignore_index = 255\n",
    "void_classes= [0,1,2,3,4,5,6,9,10,14,15,16,18,29,30,-1]\n",
    "valid_classes= [ignore_index, 7,8,11,12,13,17,19,20,21,22,23,24,25,26,27,28,31,32,33]\n",
    "class_names = ['unlabeled',\n",
    "               'road',\n",
    "               'sidewalk',\n",
    "               'building',\n",
    "               'wall',\n",
    "               'fence',\n",
    "               'pole',\n",
    "               'traffic light',\n",
    "               'traffic sign',\n",
    "               'vegetation',\n",
    "               'terrain',\n",
    "               'sky',\n",
    "               'person',\n",
    "               'rider',\n",
    "               'car',\n",
    "               'truck',\n",
    "               'bus',\n",
    "               'train',\n",
    "               'motorcycle',\n",
    "               'bicycle']\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes = len(valid_classes)\n",
    "\n",
    "#in later stage we might want to get the segements as different color coded so this is for it\n",
    "colors =[\n",
    "    [  0,   0,   0],\n",
    "    [128,  64, 128],\n",
    "    [244,  35, 232],\n",
    "    [220,  20,  60],\n",
    "    [255,   0,   0],\n",
    "    [  0,   0, 142],\n",
    "    [  0,   0,  70],\n",
    "    [  0,  60, 100],\n",
    "    [  0,  80, 100],\n",
    "    [  0,   0, 230],\n",
    "    [119,  11,  32],\n",
    "]\n",
    "\n",
    "label_colors = dict(zip(range(n_classes), colors))\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    '''\n",
    "    online mila tha \n",
    "    remove unwanted classes and rectify the labels of wanted classes\n",
    "    '''\n",
    "    for void_c in void_classes:\n",
    "        mask[mask == void_c] = ignore_index\n",
    "    for valid_c in valid_classes:\n",
    "        mask[mask == valid_c] = class_map[valid_c]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def decode_segmap(temp):\n",
    "    '''\n",
    "    ye bhi online mila tha\n",
    "    convert greyscale to color\n",
    "    ye to use nahi kara hai \n",
    "    '''\n",
    "    temp = temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colors[l][0] \n",
    "        g[temp == l] = label_colors[l][1] \n",
    "        b[temp == l] = label_colors[l][2]\n",
    "    \n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = r/255.0 \n",
    "    rgb[:,:,1] = g/255.0 \n",
    "    rgb[:,:,2] = b/255.0 \n",
    "    \n",
    "    return rgb\n",
    "\n",
    "\n",
    "\n",
    "class AdjustGamma:\n",
    "    '''\n",
    "    image bohot dark aa rahi thi to ye laga diya hai\n",
    "    thoda washed ho gayi hai image par iski wajah se\n",
    "    '''\n",
    "    def __init__(self, gamma, gain=1):\n",
    "        self.gamma = gamma\n",
    "        self.gain = gain\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        img = np.transpose(image,(2,0,1))\n",
    "        gamma_tensor = torchvision.transforms.functional.adjust_gamma(torch.from_numpy(img), self.gamma, self.gain)\n",
    "        img = np.transpose(gamma_tensor.numpy(), (1,2,0))\n",
    "        return {'image': img, 'mask': mask}\n",
    "\n",
    "class SpikeEncoding:\n",
    "    '''\n",
    "    rate coding ka code\n",
    "    '''\n",
    "    def __call__(self, image, mask):\n",
    "        image = image.float()  # Convert to float\n",
    "        mask = mask.long()    # Convert to float\n",
    "        out_img = spikegen.rate(image, num_steps=num_steps)\n",
    "        #out_mask = spikegen.rate(mask, num_steps=num_steps)\n",
    "        # out_img = out_img.bool()      #IMPORTNAT BUT NOT IMPLEMENTED\n",
    "        # out_mask = out_mask.bool()\n",
    "        return {'image': out_img, 'mask': mask}\n",
    "    \n",
    "class normalizeSeg:\n",
    "    '''\n",
    "    segmap me pata nahi normalized values nahi aa rahi thi\n",
    "    to ye ek alag se bhi bana diya \n",
    "    '''\n",
    "    def __call__(self, image, mask):\n",
    "        normalized_seg = (mask-torch.min(mask))/(torch.max(mask)-torch.min(mask))\n",
    "        return {'image': image, 'mask': normalized_seg}\n",
    "\n",
    "class encode:\n",
    "    '''\n",
    "    upar wala encode segmap function use kara hai mask par\n",
    "    '''\n",
    "    def __call__(self, image, mask):\n",
    "        final = encode_segmap(mask)\n",
    "        return {'image': image, 'mask': final}\n",
    "'''\n",
    "yaha par transforms hai \n",
    "'''\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(224,224),\n",
    "        AdjustGamma(gamma=0.63),\n",
    "        A.Normalize(mean = (0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225), max_pixel_value = float(225)),\n",
    "        ToTensorV2(),\n",
    "        encode(),\n",
    "        normalizeSeg(),\n",
    "        SpikeEncoding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "class data_transform(Cityscapes):\n",
    "    '''\n",
    "    isko samajhne ki koi zarurat nahi \n",
    "    waise bhi source code uthaya hai bas\n",
    "    par ye wala dono (image, segmap ) ko return karta hai ek saath\n",
    "    '''\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        \n",
    "        targets: Any = []\n",
    "        for i,t in enumerate(self.target_type):\n",
    "            if t == 'polygon':\n",
    "                target = self.load_json(self.targets[index][i])\n",
    "            else:\n",
    "                target = Image.open(self.targets[index][i])\n",
    "            targets.append(target)\n",
    "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
    "        \n",
    "\n",
    "        if self.transforms is not None :\n",
    "            transformed=transform(image=np.array(image), mask=np.array(target))\n",
    "            return transformed['image'], transformed['mask']\n",
    "        return image, target\n",
    "\n",
    "'''\n",
    "aise data aa ajyega\n",
    "\n",
    "dataset = data_transform('/media/iitp/ACER DATA1/cityscapes', split='val', mode='fine', target_type='semantic', transforms=transform)\n",
    "img, seg = dataset[20]\n",
    "print(img.shape, seg.shape)\n",
    "\n",
    "print(img.sum())\n",
    "print(seg.sum())\n",
    "print(img.dtype)\n",
    "print(seg.dtype)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "ch=['r','g','b']\n",
    "for i in range (0,3):\n",
    "    img_sample = img[:, i]\n",
    "    #print(img.size())\n",
    "    fig, ax = plt.subplots()\n",
    "    anim = splt.animator(img_sample, fig, ax)\n",
    "    HTML(anim.to_html5_video())\n",
    "    anim.save(f\"spike_mnist_{ch[i]}.gif\")\n",
    "\n",
    "\n",
    "mask_sample = seg[:]\n",
    "fig, ax = plt.subplots()\n",
    "anim = splt.animator(mask_sample, fig, ax)\n",
    "HTML(anim.to_html5_video())\n",
    "anim.save(f\"mask_sample.gif\")\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class CustomCollate:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, batch):\n",
    "       # Assuming batch is a list of tensors [image, mask]\n",
    "       images = [item[0] for item in batch]\n",
    "       masks = [item[1] for item in batch]\n",
    "       \n",
    "       # Stack images and masks along dimension 1\n",
    "       images_stacked = torch.stack(images, dim=1)\n",
    "       masks_stacked = torch.stack(masks, dim=1)\n",
    "       \n",
    "       return [images_stacked, masks_stacked]\n",
    "\n",
    "\n",
    "class GetData(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self, stage= None):\n",
    "        self.train_dataset = data_transform(root='../../cityscapes', split='train', mode='fine', target_type='semantic', transforms=transform)\n",
    "        self.val_dataset = data_transform(root='../../cityscapes', split='val', mode='fine', target_type='semantic', transforms=transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=CustomCollate(batch_size=batch_size))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=CustomCollate(batch_size=batch_size))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader ready\n",
      " number of batches = 186\n",
      "val_dataloader ready\n",
      " number of batches = 32\n",
      "epoch started\n",
      "batch loss \t batch accuracy 2.9957315921783447 0.9906479193239796\n",
      "batch loss \t batch accuracy 2.9957315921783447 0.9835877710459183\n",
      "batch loss \t batch accuracy 2.9957315921783447 0.9510909099968112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m total_batch_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m#for getting the summation of all batches accuracy in an epoch\u001b[39;00m\n\u001b[0;32m     55\u001b[0m total_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m#for getting the summation of all batches loss in an epoch\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[15], line 179\u001b[0m, in \u001b[0;36mdata_transform.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    175\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(targets) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(targets) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m targets[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[1;32m--> 179\u001b[0m     transformed\u001b[38;5;241m=\u001b[39m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\albumentations\\core\\composition.py:213\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[1;32m--> 213\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_each_transform:\n\u001b[0;32m    216\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "import torch\n",
    "import torch.utils.tensorboard as tb\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "GetData_obj = GetData(batch_size=batch_size)\n",
    "GetData_obj.setup()\n",
    "train_loader = GetData_obj.train_dataloader()\n",
    "val_loader = GetData_obj.val_dataloader()\n",
    "\n",
    "\n",
    "print(f\"train_dataloader ready\\n number of batches = {len(train_loader)}\")\n",
    "print(f\"val_dataloader ready\\n number of batches = {len(val_loader)}\")\n",
    "\n",
    "model = SegFormer(\n",
    "    steps= 4,\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=20,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "train_accuracy_hist = [] #for storing accuracies\n",
    "train_loss_hist = [] #for storing losses\n",
    "\n",
    "val_accuracy_hist = []#for storing the test set accuracies\n",
    "val_loss_hist = []#for storing val losses\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    print(\"epoch started\")\n",
    "    start_time = time.time() #i guess it is for showing the final time taken\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0 \n",
    "    train_acc = 0\n",
    "    \n",
    "    num_batches=0\n",
    "    \n",
    "    total_batch_acc = 0 #for getting the summation of all batches accuracy in an epoch\n",
    "    total_batch_loss = 0 #for getting the summation of all batches loss in an epoch\n",
    "    \n",
    "    for img, label in iter(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        img = model(img)\n",
    "        img = img.reshape(img.size(0), -1, img.size(2))#t,b*h*w,c\n",
    "        target = label.reshape(-1)#b*h*w\n",
    "        loss = loss_fn(img,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = SF.accuracy_rate(img,target)\n",
    "        functional.reset_net(model)\n",
    "        \n",
    "        print(\"batch loss \\t batch accuracy\", loss.item(), acc)\n",
    "        total_batch_acc += acc\n",
    "        total_batch_loss += loss.item()\n",
    "        num_batches+=1\n",
    "\n",
    "        \n",
    "    avg_epoch_acc = total_batch_acc/num_batches\n",
    "    avg_epoch_loss = total_batch_loss/num_batches\n",
    "    print(\"avg_epoch_loss \\t avg_epoch_acc\", avg_epoch_loss, avg_epoch_acc)\n",
    "    train_accuracy_hist.append(avg_epoch_acc)\n",
    "    train_loss_hist.append(avg_epoch_loss) \n",
    "    \n",
    "    \n",
    "    total_val_acc = 0\n",
    "    total_val_loss= 0\n",
    "    num_val_batches = 0 \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loader = iter(val_loader)\n",
    "        for img, label in val_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            img = model(img)\n",
    "            total_val_acc += SF.accuracy_rate(img, label)\n",
    "            total_val_loss += loss_fn(img,label)\n",
    "            num_val_batches+=1\n",
    "            \n",
    "        avg_val_acc = total_val_acc/num_val_batches\n",
    "        avg_val_loss = total_val_loss/num_val_batches\n",
    "        print(\"val set accuracy:\", avg_val_acc) \n",
    "        print(\"val set loss:\", avg_val_loss) \n",
    "        val_accuracy_hist.append(avg_val_acc)\n",
    "        val_loss_hist.append(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import functional as SF\n",
    "# from snntorch import utils\n",
    "import torch\n",
    "import torch.utils.tensorboard as tb\n",
    "import numpy as np\n",
    "# import time\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class SegFormerModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SegFormer(\n",
    "            steps=20,\n",
    "            in_channels=3,\n",
    "            widths=[64, 128, 256, 512],\n",
    "            depths=[3, 4, 6, 3],\n",
    "            all_num_heads=[1, 2, 4, 8],\n",
    "            patch_sizes=[7, 3, 3, 3],\n",
    "            overlap_sizes=[4, 2, 2, 2],\n",
    "            reduction_ratios=[8, 4, 2, 1],\n",
    "            mlp_expansions=[4, 4, 4, 4],\n",
    "            decoder_channels=256,\n",
    "            scale_factors=[8, 4, 2, 1],\n",
    "            num_classes=20,\n",
    "        )\n",
    "        #self.criterion = smp.losses.DiceLoss(mode='multiclass')\n",
    "        #self.metrics = torchmetrics.JaccardIndex(num_classes=n_classes, task='multiclass')\n",
    "        self.loss_fn= SF.ce_rate_loss()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def process(self, image, target):\n",
    "        img=self(image)\n",
    "        img = img.reshape(img.size(0), -1, img.size(2))#t,b*h*w,c\n",
    "        target = target.reshape(-1)#b*h*w\n",
    "        loss = self.loss_fn(img,target).requires_grad_(True)\n",
    "        acc= SF.accuracy_rate(img, target)\n",
    "        functional.reset_net(self.model)\n",
    "        return loss, acc\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.0001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor' : 'val_loss'}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, segment = batch\n",
    "        loss, acc = self.process(image, segment)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=image.size(1))\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, batch_size=image.size(1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, segment = batch\n",
    "        loss, acc = self.process(image, segment)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegFormerModel()\n",
    "datamodule = GetData(batch_size=16)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='checkpoints', filename='file', save_last = True)\n",
    "# Define the early stopping callback\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | SegFormer | 19.0 M\n",
      "------------------------------------\n",
      "19.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.0 M    Total params\n",
      "76.043    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  12%|█▏        | 22/186 [10:38<1:19:17,  0.03it/s, v_num=4, train_loss_step=3.000, train_acc_step=0.964]"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'd:/SpikingSegFromer/spiking jelly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Loading the best model from checkpoint\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mSegFormerModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Assuming you have trained your model and it's stored in the variable `best_model`\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Define the file path where you want to save the model weights\u001b[39;00m\n\u001b[0;32m     21\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspiking_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\core\\module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1579\u001b[0m \n\u001b[0;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\core\\saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[0;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[0;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:55\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[0;32m     52\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     )\n\u001b[0;32m     54\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\fsspec\\spec.py:1293\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1293\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\fsspec\\implementations\\local.py:184\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\fsspec\\implementations\\local.py:306\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\fsspec\\implementations\\local.py:311\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    313\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'd:/SpikingSegFromer/spiking jelly'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "tb_logger = TensorBoardLogger(\"logs/\", name = \"Spiking_seg_former\")\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=10,\n",
    "                  accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  num_sanity_val_steps=0,\n",
    "                  logger = tb_logger,\n",
    "                  )\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "# Loading the best model from checkpoint\n",
    "best_model = SegFormerModel.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n",
    "# Assuming you have trained your model and it's stored in the variable `best_model`\n",
    "\n",
    "# Define the file path where you want to save the model weights\n",
    "weights_path = \"spiking_weights.pth\"\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(best_model.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
